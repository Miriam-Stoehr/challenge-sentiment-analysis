{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import praw\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from prawcore.exceptions import RequestException # To catch rate limit exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"config.json\"\n",
    "if os.path.exists(config_file):\n",
    "    with open(config_file) as file:\n",
    "        config = json.load(file)\n",
    "        if 'client_id' in config:\n",
    "            client_id = config['client_id']\n",
    "            client_secret = config['client_secret']\n",
    "            user_agent = config['user_agent']\n",
    "            username = config['username']\n",
    "            password = config['password']\n",
    "        else:\n",
    "            raise KeyError(\"API credentials not found in config file.\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Config file {config_file} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Reddit API client\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent,\n",
    "    username=username,\n",
    "    password=password,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: Access works only with username, not with email address!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the subreddit and the number of posts to fetch\n",
    "subreddit_name = 'queensgambit'\n",
    "post_limit = 10000\n",
    "\n",
    "# Fetch posts from the subreddit\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "posts = subreddit.new(limit=post_limit) # .new fetches the most recent submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 967 posts and 6574 comments (Total: 7541).\r"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the data\n",
    "reddit_data = []\n",
    "post_counter = 0\n",
    "comment_counter = 0\n",
    "# Process the posts\n",
    "for post in posts:\n",
    "    try:\n",
    "        # Convert the Unix timestamp to a timezone-aware datetime\n",
    "        created_time = datetime.fromtimestamp(post.created_utc, tz=timezone.utc)\n",
    "        # Format the datetime as a string\n",
    "        created_time_str = created_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        post_data = {\n",
    "            \"title\": post.title,\n",
    "            \"score\": post.score,\n",
    "            \"id\": post.id,\n",
    "            \"url\": post.url,\n",
    "            \"created\": created_time_str,\n",
    "            \"text\": post.selftext,\n",
    "            \"num_comments\": post.num_comments,\n",
    "            \"comments\": []\n",
    "\n",
    "        }\n",
    "\n",
    "        '''print(f\"Title: {post.title}\")\n",
    "        print(f\"Score: {post.score}\")\n",
    "        print(f\"ID: {post.id}\")\n",
    "        print(f\"URL: {post.url}\")\n",
    "        print(f\"Created: {created_time_str}\")\n",
    "        print(f\"Text: {post.selftext}\")\n",
    "        print(f\"Number of Comments: {post.num_comments}\")\n",
    "        print(\"-\" * 80)'''\n",
    "\n",
    "        # Fetch comments\n",
    "        post.comments.replace_more(limit=0)  # Ensures that all 'MoreComments' objects are replaced, allowing access to all comments\n",
    "\n",
    "        for comment in post.comments.list():\n",
    "            comment_time = datetime.fromtimestamp(comment.created_utc, tz=timezone.utc)\n",
    "            comment_time_str = comment_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            # Check, if author exists\n",
    "            if comment.author:\n",
    "                author = comment.author.name\n",
    "                # Attempt to fetch author's comment karma\n",
    "                try:\n",
    "                    author_karma = comment.author.comment_karma\n",
    "                except AttributeError:\n",
    "                    author_karma = None\n",
    "            else:\n",
    "                author_karma = None\n",
    "                author = \"Deleted\"\n",
    "\n",
    "            comment_data = {\n",
    "                \"comment_id\": comment.id,\n",
    "                \"post_id\": post.id,             # To link the comment to its parent post\n",
    "                \"comment_text\": comment.body,\n",
    "                \"comment_score\": comment.score,\n",
    "                \"comment_created\": comment_time_str,\n",
    "                \"comment_author\": author,\n",
    "                \"author_karma\": author_karma\n",
    "            }\n",
    "\n",
    "            '''print(f\"  Comment ID: {comment.id}\")\n",
    "            print(f\"  Comment Text: {comment.body}\")\n",
    "            print(f\"  Comment Score: {comment.score}\")\n",
    "            print(f\"  Comment Created: {comment_time_str}\")\n",
    "            print(f\"  Comment Author: {comment.author}\")\n",
    "            print(f\"  Comment Author: {author}\")\n",
    "            print(f\"  Author Karma: {author_karma}\")\n",
    "            print(\"-\" * 80)'''\n",
    "\n",
    "            post_data[\"comments\"].append(comment_data)\n",
    "            comment_counter += 1\n",
    "\n",
    "        reddit_data.append(post_data)\n",
    "        post_counter += 1\n",
    "        print(f\"Processed {post_counter} posts and {comment_counter} comments (Total: {post_counter + comment_counter}).\", end=\"\\r\")\n",
    "    \n",
    "    except RequestException as e:\n",
    "        print(f\"Rate limit error occurred: {e}\")\n",
    "        print(\"Waiting before retrying...\")\n",
    "        time.sleep(60)\n",
    "\n",
    "    except praw.exceptions.PRAWException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        time.sleep(60)\n",
    "    \n",
    "    # Save the data to a JSON file\n",
    "    with open(\"reddit_data.json\", \"w\") as outfile:\n",
    "        json.dump(reddit_data, outfile, indent=4)\n",
    "    \n",
    "    time.sleep(2)  # Sleep for 2 seconds to avoid hitting the Reddit API rate limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>created</th>\n",
       "      <th>text</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Late to Play</td>\n",
       "      <td>43</td>\n",
       "      <td>1futic9</td>\n",
       "      <td>https://www.reddit.com/r/queensgambit/comments...</td>\n",
       "      <td>2024-10-02 23:00:44</td>\n",
       "      <td>I know Iâ€™m a bit late to the game, but The Que...</td>\n",
       "      <td>13</td>\n",
       "      <td>[{'comment_id': 'lq7ugjb', 'comment_text': 'I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Funny question</td>\n",
       "      <td>10</td>\n",
       "      <td>1fs558p</td>\n",
       "      <td>https://www.reddit.com/r/queensgambit/comments...</td>\n",
       "      <td>2024-09-29 14:06:51</td>\n",
       "      <td>So I have OCD and one of my triggers is people...</td>\n",
       "      <td>14</td>\n",
       "      <td>[{'comment_id': 'lphutpg', 'comment_text': 'So...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alma and Beth</td>\n",
       "      <td>50</td>\n",
       "      <td>1fqc156</td>\n",
       "      <td>https://www.reddit.com/r/queensgambit/comments...</td>\n",
       "      <td>2024-09-27 01:07:02</td>\n",
       "      <td>Just an appreciation post for Alma and Beth's ...</td>\n",
       "      <td>11</td>\n",
       "      <td>[{'comment_id': 'lp4bmil', 'comment_text': 'Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beth will forever live rent free in my headðŸ¥°</td>\n",
       "      <td>319</td>\n",
       "      <td>1fojobd</td>\n",
       "      <td>https://www.reddit.com/gallery/1fojobd</td>\n",
       "      <td>2024-09-24 18:31:14</td>\n",
       "      <td></td>\n",
       "      <td>20</td>\n",
       "      <td>[{'comment_id': 'lorh42m', 'comment_text': 'Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Queen gambit</td>\n",
       "      <td>7</td>\n",
       "      <td>1foc7bl</td>\n",
       "      <td>https://www.reddit.com/r/queensgambit/comments...</td>\n",
       "      <td>2024-09-24 13:14:50</td>\n",
       "      <td>I just started watching it is it worth it?</td>\n",
       "      <td>9</td>\n",
       "      <td>[{'comment_id': 'lopun1k', 'comment_text': 'Ye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  score       id  \\\n",
       "0                                  Late to Play     43  1futic9   \n",
       "1                                Funny question     10  1fs558p   \n",
       "2                                 Alma and Beth     50  1fqc156   \n",
       "3  Beth will forever live rent free in my headðŸ¥°    319  1fojobd   \n",
       "4                                  Queen gambit      7  1foc7bl   \n",
       "\n",
       "                                                 url              created  \\\n",
       "0  https://www.reddit.com/r/queensgambit/comments...  2024-10-02 23:00:44   \n",
       "1  https://www.reddit.com/r/queensgambit/comments...  2024-09-29 14:06:51   \n",
       "2  https://www.reddit.com/r/queensgambit/comments...  2024-09-27 01:07:02   \n",
       "3             https://www.reddit.com/gallery/1fojobd  2024-09-24 18:31:14   \n",
       "4  https://www.reddit.com/r/queensgambit/comments...  2024-09-24 13:14:50   \n",
       "\n",
       "                                                text  num_comments  \\\n",
       "0  I know Iâ€™m a bit late to the game, but The Que...            13   \n",
       "1  So I have OCD and one of my triggers is people...            14   \n",
       "2  Just an appreciation post for Alma and Beth's ...            11   \n",
       "3                                                               20   \n",
       "4         I just started watching it is it worth it?             9   \n",
       "\n",
       "                                            comments  \n",
       "0  [{'comment_id': 'lq7ugjb', 'comment_text': 'I ...  \n",
       "1  [{'comment_id': 'lphutpg', 'comment_text': 'So...  \n",
       "2  [{'comment_id': 'lp4bmil', 'comment_text': 'Th...  \n",
       "3  [{'comment_id': 'lorh42m', 'comment_text': 'Th...  \n",
       "4  [{'comment_id': 'lopun1k', 'comment_text': 'Ye...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load JSON data\n",
    "with open(\"reddit_data.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Normalize the JSON data\n",
    "posts_df = pd.json_normalize(data)\n",
    "\n",
    "# Normalize comments data\n",
    "comments_data = []\n",
    "for post in data:\n",
    "    for comment in post['comments']:\n",
    "        #comment['post_id'] = post['post_id']\n",
    "        comments_data.append(comment)\n",
    "\n",
    "comments_df = pd.DataFrame(comments_data)\n",
    "\n",
    "# Display DataFrames\n",
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>comment_score</th>\n",
       "      <th>comment_created</th>\n",
       "      <th>comment_author</th>\n",
       "      <th>author_karma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lq7ugjb</td>\n",
       "      <td>I recommend watching the Queens Gambit again</td>\n",
       "      <td>10</td>\n",
       "      <td>2024-10-03 23:19:30</td>\n",
       "      <td>SirZacharia</td>\n",
       "      <td>50248.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lq24ob7</td>\n",
       "      <td>The Queenâ€™s Gambit is soooooo good!!  I recomm...</td>\n",
       "      <td>4</td>\n",
       "      <td>2024-10-02 23:48:00</td>\n",
       "      <td>Lost_As_Alice_</td>\n",
       "      <td>14735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lq3ot0r</td>\n",
       "      <td>Itâ€™s not a series, but if you liked her (canâ€™t...</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-10-03 07:04:53</td>\n",
       "      <td>ButterflyBelleFL</td>\n",
       "      <td>2546.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lqq8uv0</td>\n",
       "      <td>Iâ€™m reading the book before watching the show,...</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-10-07 04:06:15</td>\n",
       "      <td>I-Am-Baldy</td>\n",
       "      <td>981.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lqaqh57</td>\n",
       "      <td>I like Searching for Bobby Fischer.\\n\\n-Seven-...</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-10-04 13:31:55</td>\n",
       "      <td>Numerous-Editor-8778</td>\n",
       "      <td>228.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comment_id                                       comment_text  \\\n",
       "0    lq7ugjb       I recommend watching the Queens Gambit again   \n",
       "1    lq24ob7  The Queenâ€™s Gambit is soooooo good!!  I recomm...   \n",
       "2    lq3ot0r  Itâ€™s not a series, but if you liked her (canâ€™t...   \n",
       "3    lqq8uv0  Iâ€™m reading the book before watching the show,...   \n",
       "4    lqaqh57  I like Searching for Bobby Fischer.\\n\\n-Seven-...   \n",
       "\n",
       "   comment_score      comment_created        comment_author  author_karma  \n",
       "0             10  2024-10-03 23:19:30           SirZacharia       50248.0  \n",
       "1              4  2024-10-02 23:48:00        Lost_As_Alice_       14735.0  \n",
       "2              3  2024-10-03 07:04:53      ButterflyBelleFL        2546.0  \n",
       "3              3  2024-10-07 04:06:15            I-Am-Baldy         981.0  \n",
       "4              1  2024-10-04 13:31:55  Numerous-Editor-8778         228.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Subreddit 'Netflix' Query 'Queen's Gambit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect  # Import langdetect\n",
    "\n",
    "# Define subreddit and search query\n",
    "subreddit = reddit.subreddit('netflix')\n",
    "query = 'Queen\\'s Gambit'\n",
    "\n",
    "# Scrape posts\n",
    "posts = []\n",
    "for submission in subreddit.search(query, limit=10000, sort='new'):\n",
    "    try:\n",
    "        # Detect the language of the post text\n",
    "        if detect(submission.title + ' ' + submission.selftext) == 'en':\n",
    "            posts.append(submission.title + ' ' + submission.selftext)\n",
    "    except Exception as e:\n",
    "        # In case of a detection error, skip the post\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Processed 138 posts and 3671 comments (Total: 3809).\r"
     ]
    }
   ],
   "source": [
    "from langdetect import detect  # Import langdetect\n",
    "\n",
    "# Define subreddit and search query\n",
    "subreddit = reddit.subreddit('netflix')\n",
    "query = 'Queen\\'s Gambit'\n",
    "\n",
    "# Scrape posts\n",
    "posts_data = []\n",
    "\n",
    "# Initialize counters\n",
    "post_counter = 0\n",
    "comment_counter = 0\n",
    "\n",
    "for submission in subreddit.search(query, limit=10000, sort='new'):\n",
    "    try:\n",
    "        # Detect the language of the post text\n",
    "        if detect(submission.title + ' ' + submission.selftext) == 'en':\n",
    "            try:\n",
    "\n",
    "                # Convert the Unix timestamp to a timezone-aware datetime\n",
    "                created_time = datetime.fromtimestamp(submission.created_utc, tz=timezone.utc)\n",
    "                created_time_str = created_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                # Store post information\n",
    "                post_data = {\n",
    "                    \"title\": submission.title,\n",
    "                    \"score\": submission.score,\n",
    "                    \"id\": submission.id,\n",
    "                    \"url\": submission.url,\n",
    "                    \"created\": created_time_str,\n",
    "                    \"text\": submission.selftext,\n",
    "                    \"num_comments\": submission.num_comments,\n",
    "                    \"comments\": []  # Will be filled with comment data\n",
    "                }\n",
    "\n",
    "                # Fetch comments\n",
    "                submission.comments.replace_more(limit=0)  # Ensures all comments are fetched\n",
    "                for comment in submission.comments.list():\n",
    "                    comment_time = datetime.fromtimestamp(comment.created_utc, tz=timezone.utc)\n",
    "                    comment_time_str = comment_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                    # Check if comment has an author\n",
    "                    if comment.author:\n",
    "                        author = comment.author.name\n",
    "                        try:\n",
    "                            author_karma = comment.author.comment_karma\n",
    "                        except AttributeError:\n",
    "                            author_karma = None\n",
    "                    else:\n",
    "                        author = \"Deleted\"\n",
    "                        author_karma = None\n",
    "\n",
    "                    # Store comment information\n",
    "                    comment_data = {\n",
    "                        \"comment_id\": comment.id,\n",
    "                        \"post_id\": submission.id,  # Link comment to its parent post\n",
    "                        \"comment_text\": comment.body,\n",
    "                        \"comment_score\": comment.score,\n",
    "                        \"comment_created\": comment_time_str,\n",
    "                        \"comment_author\": author,\n",
    "                        \"author_karma\": author_karma\n",
    "                    }\n",
    "\n",
    "                    # Append comment data to post's comment list\n",
    "                    post_data[\"comments\"].append(comment_data)\n",
    "\n",
    "                    # Increment comment counter\n",
    "                    comment_counter += 1\n",
    "\n",
    "                # Append the post data with its comments to the main list\n",
    "                posts_data.append(post_data)\n",
    "                # Increment post counter\n",
    "                post_counter += 1\n",
    "                print(f\"Processed {post_counter} posts and {comment_counter} comments (Total: {post_counter + comment_counter}).\", end=\"\\r\")\n",
    "            \n",
    "            except RequestException as e:\n",
    "                print(f\"Rate limit error occurred: {e}\")\n",
    "                print(\"Waiting before retrying...\")\n",
    "                time.sleep(60)\n",
    "            \n",
    "            # Save the data to a JSON file\n",
    "            with open(\"netflix_reddit_data.json\", \"w\") as outfile:\n",
    "                json.dump(posts_data, outfile, indent=4)\n",
    "            \n",
    "            # Sleep for 2 seconds to avoid hitting the Reddit API rate limit\n",
    "            time.sleep(2)\n",
    "\n",
    "    except Exception as e:\n",
    "        # In case of a detection error, skip the post\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data\n",
    "with open(\"netflix_reddit_data.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Normalize the JSON data\n",
    "nf_posts_df = pd.json_normalize(data)\n",
    "\n",
    "# Normalize comments data\n",
    "comments_data = []\n",
    "for post in data:\n",
    "    for comment in post['comments']:\n",
    "        #comment['post_id'] = post['post_id']\n",
    "        comments_data.append(comment)\n",
    "\n",
    "nf_comments_df = pd.DataFrame(comments_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Subreddit 'Television' Query 'Queen's Gambit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Error detecting language: received 429 HTTP response\n",
      "Processed 79 posts and 6483 comments (Total: 6562).\r"
     ]
    }
   ],
   "source": [
    "from langdetect import detect  # Import langdetect\n",
    "\n",
    "# Define subreddit and search query\n",
    "subreddit = reddit.subreddit('television')\n",
    "query = 'Queen\\'s Gambit'\n",
    "\n",
    "# Scrape posts\n",
    "posts_data = []\n",
    "\n",
    "# Initialize counters\n",
    "post_counter = 0\n",
    "comment_counter = 0\n",
    "\n",
    "for submission in subreddit.search(query, limit=10000, sort='new'):\n",
    "    try:\n",
    "        # Detect the language of the post text\n",
    "        if detect(submission.title + ' ' + submission.selftext) == 'en':\n",
    "            try:\n",
    "\n",
    "                # Convert the Unix timestamp to a timezone-aware datetime\n",
    "                created_time = datetime.fromtimestamp(submission.created_utc, tz=timezone.utc)\n",
    "                created_time_str = created_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                # Store post information\n",
    "                post_data = {\n",
    "                    \"title\": submission.title,\n",
    "                    \"score\": submission.score,\n",
    "                    \"id\": submission.id,\n",
    "                    \"url\": submission.url,\n",
    "                    \"created\": created_time_str,\n",
    "                    \"text\": submission.selftext,\n",
    "                    \"num_comments\": submission.num_comments,\n",
    "                    \"comments\": []  # Will be filled with comment data\n",
    "                }\n",
    "\n",
    "                # Fetch comments\n",
    "                submission.comments.replace_more(limit=0)  # Ensures all comments are fetched\n",
    "                for comment in submission.comments.list():\n",
    "                    comment_time = datetime.fromtimestamp(comment.created_utc, tz=timezone.utc)\n",
    "                    comment_time_str = comment_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                    # Check if comment has an author\n",
    "                    if comment.author:\n",
    "                        author = comment.author.name\n",
    "                        try:\n",
    "                            author_karma = comment.author.comment_karma\n",
    "                        except AttributeError:\n",
    "                            author_karma = None\n",
    "                    else:\n",
    "                        author = \"Deleted\"\n",
    "                        author_karma = None\n",
    "\n",
    "                    # Store comment information\n",
    "                    comment_data = {\n",
    "                        \"comment_id\": comment.id,\n",
    "                        \"post_id\": submission.id,  # Link comment to its parent post\n",
    "                        \"comment_text\": comment.body,\n",
    "                        \"comment_score\": comment.score,\n",
    "                        \"comment_created\": comment_time_str,\n",
    "                        \"comment_author\": author,\n",
    "                        \"author_karma\": author_karma\n",
    "                    }\n",
    "\n",
    "                    # Append comment data to post's comment list\n",
    "                    post_data[\"comments\"].append(comment_data)\n",
    "\n",
    "                    # Increment comment counter\n",
    "                    comment_counter += 1\n",
    "\n",
    "                # Append the post data with its comments to the main list\n",
    "                posts_data.append(post_data)\n",
    "                # Increment post counter\n",
    "                post_counter += 1\n",
    "                print(f\"Processed {post_counter} posts and {comment_counter} comments (Total: {post_counter + comment_counter}).\", end=\"\\r\")\n",
    "            \n",
    "            except RequestException as e:\n",
    "                print(f\"Rate limit error occurred: {e}\")\n",
    "                print(\"Waiting before retrying...\")\n",
    "                time.sleep(60)\n",
    "            \n",
    "            # Save the data to a JSON file\n",
    "            with open(\"television_reddit_data.json\", \"w\") as outfile:\n",
    "                json.dump(posts_data, outfile, indent=4)\n",
    "            \n",
    "            # Sleep for 2 seconds to avoid hitting the Reddit API rate limit\n",
    "            time.sleep(2)\n",
    "\n",
    "    except Exception as e:\n",
    "        # In case of a detection error, skip the post\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Subreddit 'NetflixBestOf' Query 'Queen's Gambit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect  # Import langdetect\n",
    "\n",
    "# Define subreddit and search query\n",
    "subreddit = reddit.subreddit('NetflixBestOf')\n",
    "query = 'Queen\\'s Gambit'\n",
    "\n",
    "# Scrape posts\n",
    "posts_data = []\n",
    "\n",
    "# Initialize counters\n",
    "post_counter = 0\n",
    "comment_counter = 0\n",
    "\n",
    "for submission in subreddit.search(query, limit=10000, sort='new'):\n",
    "    try:\n",
    "        # Detect the language of the post text\n",
    "        if detect(submission.title + ' ' + submission.selftext) == 'en':\n",
    "            try:\n",
    "\n",
    "                # Convert the Unix timestamp to a timezone-aware datetime\n",
    "                created_time = datetime.fromtimestamp(submission.created_utc, tz=timezone.utc)\n",
    "                created_time_str = created_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                # Store post information\n",
    "                post_data = {\n",
    "                    \"title\": submission.title,\n",
    "                    \"score\": submission.score,\n",
    "                    \"id\": submission.id,\n",
    "                    \"url\": submission.url,\n",
    "                    \"created\": created_time_str,\n",
    "                    \"text\": submission.selftext,\n",
    "                    \"num_comments\": submission.num_comments,\n",
    "                    \"comments\": []  # Will be filled with comment data\n",
    "                }\n",
    "\n",
    "                # Fetch comments\n",
    "                submission.comments.replace_more(limit=0)  # Ensures all comments are fetched\n",
    "                for comment in submission.comments.list():\n",
    "                    comment_time = datetime.fromtimestamp(comment.created_utc, tz=timezone.utc)\n",
    "                    comment_time_str = comment_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                    # Check if comment has an author\n",
    "                    if comment.author:\n",
    "                        author = comment.author.name\n",
    "                        try:\n",
    "                            author_karma = comment.author.comment_karma\n",
    "                        except AttributeError:\n",
    "                            author_karma = None\n",
    "                    else:\n",
    "                        author = \"Deleted\"\n",
    "                        author_karma = None\n",
    "\n",
    "                    # Store comment information\n",
    "                    comment_data = {\n",
    "                        \"comment_id\": comment.id,\n",
    "                        \"post_id\": submission.id,  # Link comment to its parent post\n",
    "                        \"comment_text\": comment.body,\n",
    "                        \"comment_score\": comment.score,\n",
    "                        \"comment_created\": comment_time_str,\n",
    "                        \"comment_author\": author,\n",
    "                        \"author_karma\": author_karma\n",
    "                    }\n",
    "\n",
    "                    # Append comment data to post's comment list\n",
    "                    post_data[\"comments\"].append(comment_data)\n",
    "\n",
    "                    # Increment comment counter\n",
    "                    comment_counter += 1\n",
    "\n",
    "                # Append the post data with its comments to the main list\n",
    "                posts_data.append(post_data)\n",
    "                # Increment post counter\n",
    "                post_counter += 1\n",
    "                print(f\"Processed {post_counter} posts and {comment_counter} comments (Total: {post_counter + comment_counter}).\", end=\"\\r\")\n",
    "            \n",
    "            except RequestException as e:\n",
    "                print(f\"Rate limit error occurred: {e}\")\n",
    "                print(\"Waiting before retrying...\")\n",
    "                time.sleep(60)\n",
    "            \n",
    "            # Save the data to a JSON file\n",
    "            with open(\"nfbo_reddit_data.json\", \"w\") as outfile:\n",
    "                json.dump(posts_data, outfile, indent=4)\n",
    "            \n",
    "            # Sleep for 2 seconds to avoid hitting the Reddit API rate limit\n",
    "            time.sleep(2)\n",
    "\n",
    "    except Exception as e:\n",
    "        # In case of a detection error, skip the post\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Subreddit 'all' Query 'Queen's Gambit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
