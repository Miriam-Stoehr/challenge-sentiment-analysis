{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis - Part I (Web Scraping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import praw\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from langdetect import detect\n",
    "from prawcore.exceptions import RequestException # To catch rate limit exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the config file\n",
    "config_file = \"config.json\"\n",
    "if os.path.exists(config_file):\n",
    "    with open(config_file) as file:\n",
    "        config = json.load(file)\n",
    "        if 'client_id' in config:\n",
    "            client_id = config['client_id']\n",
    "            client_secret = config['client_secret']\n",
    "            user_agent = config['user_agent']\n",
    "            username = config['username']\n",
    "            password = config['password']\n",
    "        else:\n",
    "            raise KeyError(\"API credentials not found in config file.\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Config file {config_file} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Reddit API client\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent,\n",
    "    username=username,\n",
    "    password=password,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 47 posts and 354 comments (Total: 401). Current subreddit: netflix.\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prawcore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 62\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     author_karma \u001b[38;5;241m=\u001b[39m \u001b[43mcomment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomment_karma\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\praw\\models\\reddit\\base.py:38\u001b[0m, in \u001b[0;36mRedditBase.__getattr__\u001b[1;34m(self, attribute)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attribute\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetched:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attribute)\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\praw\\models\\reddit\\redditor.py:181\u001b[0m, in \u001b[0;36mRedditor._fetch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fetch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 181\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     data \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\praw\\models\\reddit\\base.py:89\u001b[0m, in \u001b[0;36mRedditBase._fetch_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     88\u001b[0m path \u001b[38;5;241m=\u001b[39m API_PATH[name]\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfields)\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reddit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\praw\\util\\deprecate_args.py:46\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m     warn(\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m will no longer be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     45\u001b[0m     )\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_old_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\praw\\reddit.py:963\u001b[0m, in \u001b[0;36mReddit.request\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BadRequest \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\prawcore\\sessions.py:328\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[0;32m    327\u001b[0m url \u001b[38;5;241m=\u001b[39m urljoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requestor\u001b[38;5;241m.\u001b[39moauth_url, path)\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\prawcore\\sessions.py:234\u001b[0m, in \u001b[0;36mSession._request_with_retries\u001b[1;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_request(data, method, params, url)\n\u001b[1;32m--> 234\u001b[0m response, saved_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_strategy_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m do_retry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\prawcore\\sessions.py:186\u001b[0m, in \u001b[0;36mSession._make_request\u001b[1;34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rate_limiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_header_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m bytes) (rst-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:rem-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:used-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m ratelimit) at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    200\u001b[0m         response\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         time\u001b[38;5;241m.\u001b[39mtime(),\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\prawcore\\rate_limit.py:47\u001b[0m, in \u001b[0;36mRateLimiter.call\u001b[1;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m set_header_callback()\n\u001b[1;32m---> 47\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(response\u001b[38;5;241m.\u001b[39mheaders)\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\prawcore\\requestor.py:68\u001b[0m, in \u001b[0;36mRequestor.request\u001b[1;34m(self, timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_http\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:  \u001b[38;5;66;03m# noqa: BLE001\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\socket.py:719\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:1304\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1302\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1303\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:1138\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 95\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;66;03m# Print progress\u001b[39;00m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpost_counter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m posts and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomment_counter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m comments (Total: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(seen_posts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Current subreddit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubreddit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mprawcore\u001b[49m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTooManyRequests \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRate limit error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting before retrying...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prawcore' is not defined"
     ]
    }
   ],
   "source": [
    "# Scraping subreddit 'queensgambit'\n",
    "\n",
    "# Define subreddits and query\n",
    "subreddit = 'queensgambit'\n",
    "\n",
    "# Create a set to store already-seen content\n",
    "seen_posts = set()\n",
    "\n",
    "# Initialize counters\n",
    "post_counter = 0\n",
    "comment_counter = 0\n",
    "\n",
    "# Initialize list to store posts\n",
    "posts = []\n",
    "\n",
    "# Fetch posts from the subreddit\n",
    "subreddit = reddit.subreddit(subreddit)\n",
    "subreddit_posts = subreddit.new(limit=15000)\n",
    "\n",
    "# Scraping loop\n",
    "for submission in subreddit_posts:\n",
    "    try:\n",
    "        # Check if the post has already been seen\n",
    "        post_identifier = submission.selftext.strip().lower()\n",
    "\n",
    "        # If the post hasn't been seen, add it\n",
    "        if post_identifier not in seen_posts:\n",
    "            seen_posts.add(post_identifier)\n",
    "            # Convert unix timestamp to timezone-aware datetime\n",
    "            created_time = datetime.fromtimestamp(submission.created_utc, tz=timezone.utc)\n",
    "            created_time_str = created_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            # Store post information\n",
    "            posts_data = {\n",
    "                \"title\": submission.title,\n",
    "                \"id\": submission.id,\n",
    "                \"text\": submission.selftext,\n",
    "                \"score\": submission.score,\n",
    "                \"url\": submission.url,\n",
    "                \"created\": created_time_str,\n",
    "                \"num_comments\": submission.num_comments,\n",
    "                \"comments\": [],                              # Placeholder for comments\n",
    "            }\n",
    "\n",
    "            # Fetch comments\n",
    "            submission.comments.replace_more(limit=0)        # Replace MoreComments objects with actual comments\n",
    "            for comment in submission.comments.list():\n",
    "\n",
    "                    # Check if the comment has been seen\n",
    "                    comment_identifier = comment.body.strip().lower()\n",
    "                    if comment_identifier not in seen_posts:\n",
    "                        seen_posts.add(comment_identifier)\n",
    "\n",
    "                        # Convert unix timestamp to timezone-aware datetime\n",
    "                        comment_time = datetime.fromtimestamp(comment.created_utc, tz=timezone.utc)\n",
    "                        comment_time_str = comment_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        \n",
    "                        # Check, if comment has an author\n",
    "                        if comment.author:\n",
    "                            author = comment.author.name\n",
    "                            try:\n",
    "                                author_karma = comment.author.comment_karma\n",
    "                            except AttributeError:\n",
    "                                author_karma = None\n",
    "                        else:\n",
    "                            author = 'Deleted'\n",
    "                            author_karma = None\n",
    "                        \n",
    "                        # Store comment information\n",
    "                        comment_data = {\n",
    "                            \"comment_id\": comment.id,\n",
    "                            \"post_id\": submission.id,\n",
    "                            \"comment_text\": comment.body,\n",
    "                            \"comment_score\": comment.score,\n",
    "                            \"comment_created\": comment_time_str,\n",
    "                            \"comment_author\": author,\n",
    "                            \"comment_author_karma\": author_karma,\n",
    "                        }\n",
    "\n",
    "                        # Append comment data to the post\n",
    "                        posts_data[\"comments\"].append(comment_data)\n",
    "\n",
    "                        # Increment comment counter\n",
    "                        comment_counter += 1\n",
    "\n",
    "            # Append post data to the list\n",
    "            posts.append(posts_data)\n",
    "\n",
    "            # Increment post counter\n",
    "            post_counter += 1\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Processed {post_counter} posts and {comment_counter} comments (Total: {len(seen_posts)}).\", end='\\r')\n",
    "        \n",
    "    except prawcore.exceptions.TooManyRequests as e:\n",
    "        print(f\"Rate limit error occurred: {e}\")\n",
    "        print(\"Waiting before retrying...\")\n",
    "        time.sleep(60)\n",
    "    \n",
    "    # Save the data to a JSON file\n",
    "    with open(\"./data/reddit_data.json\", \"w\") as outfile:\n",
    "        json.dump(posts, outfile, indent=4)\n",
    "    \n",
    "    # Sleep for 2 seconds to avoid hitting the Reddit API rate limit\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 178 posts and 2043 comments (Total: 2221). Current subreddit: netflix.mbit.\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prawcore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTooManyRequests\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 65\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     author_karma \u001b[38;5;241m=\u001b[39m \u001b[43mcomment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomment_karma\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\praw\\models\\reddit\\base.py:38\u001b[0m, in \u001b[0;36mRedditBase.__getattr__\u001b[1;34m(self, attribute)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attribute\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetched:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attribute)\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\praw\\models\\reddit\\redditor.py:181\u001b[0m, in \u001b[0;36mRedditor._fetch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fetch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 181\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     data \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\praw\\models\\reddit\\base.py:89\u001b[0m, in \u001b[0;36mRedditBase._fetch_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     88\u001b[0m path \u001b[38;5;241m=\u001b[39m API_PATH[name]\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfields)\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reddit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\praw\\util\\deprecate_args.py:46\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m     warn(\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m will no longer be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     45\u001b[0m     )\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_old_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\praw\\reddit.py:963\u001b[0m, in \u001b[0;36mReddit.request\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BadRequest \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\prawcore\\sessions.py:328\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[0;32m    327\u001b[0m url \u001b[38;5;241m=\u001b[39m urljoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requestor\u001b[38;5;241m.\u001b[39moauth_url, path)\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Dokumente\\Programming&DataScience\\BeCode\\AI & DS Bootcamp\\Content\\Projects\\sentiment-analysis\\sentiment_analysis_env\\Lib\\site-packages\\prawcore\\sessions.py:267\u001b[0m, in \u001b[0;36mSession._request_with_retries\u001b[1;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTATUS_EXCEPTIONS:\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTATUS_EXCEPTIONS[response\u001b[38;5;241m.\u001b[39mstatus_code](response)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m codes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_content\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[1;31mTooManyRequests\u001b[0m: received 429 HTTP response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 98\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;66;03m# Print progress\u001b[39;00m\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpost_counter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m posts and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomment_counter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m comments (Total: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(seen_posts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Current subreddit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubreddit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mprawcore\u001b[49m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTooManyRequests \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRate limit error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting before retrying...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prawcore' is not defined"
     ]
    }
   ],
   "source": [
    "# Define subreddits and query\n",
    "subreddits = ['netflix', 'NetflixBestOf', 'television', 'TvShows']\n",
    "query = 'Queen\\'s Gambit'\n",
    "\n",
    "# Create a set to store already-seen content\n",
    "seen_posts = set()\n",
    "\n",
    "# Initialize counters\n",
    "post_counter = 0\n",
    "comment_counter = 0\n",
    "\n",
    "# Initialize list to store posts\n",
    "posts = []\n",
    "\n",
    "# Scraping loop\n",
    "for subreddit_name in subreddits:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    # Search in the current subreddit\n",
    "    for submission in subreddit.search(query, limit=15000, sort='new'):\n",
    "        try:\n",
    "            # Check if the post has already been seen\n",
    "            post_identifier = submission.selftext.strip().lower()\n",
    "\n",
    "            # If the post hasn't been seen, add it\n",
    "            if post_identifier not in seen_posts:\n",
    "                seen_posts.add(post_identifier)\n",
    "                # Convert unix timestamp to timezone-aware datetime\n",
    "                created_time = datetime.fromtimestamp(submission.created_utc, tz=timezone.utc)\n",
    "                created_time_str = created_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                # Store post information\n",
    "                posts_data = {\n",
    "                    \"title\": submission.title,\n",
    "                    \"id\": submission.id,\n",
    "                    \"text\": submission.selftext,\n",
    "                    \"score\": submission.score,\n",
    "                    \"url\": submission.url,\n",
    "                    \"created\": created_time_str,\n",
    "                    \"num_comments\": submission.num_comments,\n",
    "                    \"comments\": [],                              # Placeholder for comments\n",
    "                }\n",
    "\n",
    "                # Fetch comments\n",
    "                submission.comments.replace_more(limit=0)        # Replace MoreComments objects with actual comments\n",
    "                for comment in submission.comments.list():\n",
    "\n",
    "                        # Check if the comment has been seen\n",
    "                        comment_identifier = comment.body.strip().lower()\n",
    "                        if comment_identifier not in seen_posts:\n",
    "                            seen_posts.add(comment_identifier)\n",
    "\n",
    "                            # Convert unix timestamp to timezone-aware datetime\n",
    "                            comment_time = datetime.fromtimestamp(comment.created_utc, tz=timezone.utc)\n",
    "                            comment_time_str = comment_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                            \n",
    "                            # Check, if comment has an author\n",
    "                            if comment.author:\n",
    "                                author = comment.author.name\n",
    "                                try:\n",
    "                                    author_karma = comment.author.comment_karma\n",
    "                                except AttributeError:\n",
    "                                    author_karma = None\n",
    "                            else:\n",
    "                                author = 'Deleted'\n",
    "                                author_karma = None\n",
    "                            \n",
    "                            # Store comment information\n",
    "                            comment_data = {\n",
    "                                \"comment_id\": comment.id,\n",
    "                                \"post_id\": submission.id,\n",
    "                                \"comment_text\": comment.body,\n",
    "                                \"comment_score\": comment.score,\n",
    "                                \"comment_created\": comment_time_str,\n",
    "                                \"comment_author\": author,\n",
    "                                \"comment_author_karma\": author_karma,\n",
    "                            }\n",
    "\n",
    "                            # Append comment data to the post\n",
    "                            posts_data[\"comments\"].append(comment_data)\n",
    "\n",
    "                            # Increment comment counter\n",
    "                            comment_counter += 1\n",
    "\n",
    "                # Append post data to the list\n",
    "                posts.append(posts_data)\n",
    "\n",
    "                # Increment post counter\n",
    "                post_counter += 1\n",
    "\n",
    "                # Print progress\n",
    "                print(f\"Processed {post_counter} posts and {comment_counter} comments (Total: {len(seen_posts)}). Current subreddit: {subreddit_name}.\", end='\\r')\n",
    "            \n",
    "        except prawcore.exceptions.TooManyRequests as e:\n",
    "            print(f\"Rate limit error occurred: {e}\")\n",
    "            print(\"Waiting before retrying...\")\n",
    "            time.sleep(60)\n",
    "        \n",
    "        # Save the data to a JSON file\n",
    "        with open(\"./data/reddit_data.json\", \"a\") as outfile:\n",
    "            json.dump(posts, outfile, indent=4)\n",
    "        \n",
    "        # Sleep for 2 seconds to avoid hitting the Reddit API rate limit\n",
    "        time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error occurred: received 429 HTTP responseSubreddit: netflix.mbit.\n",
      "Waiting before retrying...\n",
      "Rate limit error occurred: received 429 HTTP responseSubreddit: NetflixBestOf.\n",
      "Waiting before retrying...\n",
      "Rate limit error occurred: received 429 HTTP response Subreddit: NetflixBestOf.\n",
      "Waiting before retrying...\n",
      "Rate limit error occurred: received 429 HTTP response. Subreddit: NetflixBestOf.\n",
      "Waiting before retrying...\n",
      "Rate limit error occurred: received 429 HTTP response. Subreddit: NetflixBestOf.\n",
      "Waiting before retrying...\n",
      "Rate limit error occurred: received 429 HTTP response. Subreddit: NetflixBestOf.\n",
      "Waiting before retrying...\n",
      "Rate limit error occurred: received 429 HTTP response. Subreddit: NetflixBestOf.\n",
      "Waiting before retrying...\n",
      "Processed 711 posts and 17684 comments (Total: 18409). Subreddit: TvShows.on.Of.\r"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import prawcore\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Initialize the Reddit API client\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent,\n",
    "    username=username,\n",
    "    password=password,\n",
    ")\n",
    "\n",
    "# Define the subreddits and query\n",
    "subreddits = ['queensgambit', 'netflix', 'NetflixBestOf', 'television', 'TvShows']\n",
    "query = 'Queen\\'s Gambit'  # Only used for multiple subreddits\n",
    "limit = 15000\n",
    "\n",
    "# Create a set to store already-seen content\n",
    "seen_posts = set()\n",
    "\n",
    "# Initialize counters\n",
    "post_counter = 0\n",
    "comment_counter = 0\n",
    "\n",
    "# Initialize list to store posts\n",
    "posts = []\n",
    "\n",
    "# Function to scrape a subreddit\n",
    "def scrape_subreddit(subreddit_name, limit, use_query=False, query=None):\n",
    "    global post_counter, comment_counter\n",
    "\n",
    "    # Access the subreddit\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    # Fetch posts (with or without query)\n",
    "    if use_query:\n",
    "        subreddit_posts = subreddit.search(query, limit=limit, sort='new')\n",
    "    else:\n",
    "        subreddit_posts = subreddit.new(limit=limit)\n",
    "\n",
    "    # Scraping loop\n",
    "    for submission in subreddit_posts:\n",
    "        try:\n",
    "            # Check if the post has already been seen\n",
    "            post_identifier = submission.selftext.strip().lower()\n",
    "\n",
    "            # If the post hasn't been seen, add it\n",
    "            if post_identifier not in seen_posts:\n",
    "                seen_posts.add(post_identifier)\n",
    "                # Convert unix timestamp to timezone-aware datetime\n",
    "                created_time = datetime.fromtimestamp(submission.created_utc, tz=timezone.utc)\n",
    "                created_time_str = created_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                # Store post information\n",
    "                posts_data = {\n",
    "                    \"title\": submission.title,\n",
    "                    \"id\": submission.id,\n",
    "                    \"text\": submission.selftext,\n",
    "                    \"score\": submission.score,\n",
    "                    \"url\": submission.url,\n",
    "                    \"created\": created_time_str,\n",
    "                    \"num_comments\": submission.num_comments,\n",
    "                    \"comments\": [],  # Placeholder for comments\n",
    "                }\n",
    "\n",
    "                # Fetch comments\n",
    "                submission.comments.replace_more(limit=0)  # Replace MoreComments objects with actual comments\n",
    "                for comment in submission.comments.list():\n",
    "                    # Check if the comment has been seen\n",
    "                    comment_identifier = comment.body.strip().lower()\n",
    "                    if comment_identifier not in seen_posts:\n",
    "                        seen_posts.add(comment_identifier)\n",
    "\n",
    "                        # Convert unix timestamp to timezone-aware datetime\n",
    "                        comment_time = datetime.fromtimestamp(comment.created_utc, tz=timezone.utc)\n",
    "                        comment_time_str = comment_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                        # Check if the comment has an author\n",
    "                        if comment.author:\n",
    "                            author = comment.author.name\n",
    "                            try:\n",
    "                                author_karma = comment.author.comment_karma\n",
    "                            except AttributeError:\n",
    "                                author_karma = None\n",
    "                        else:\n",
    "                            author = 'Deleted'\n",
    "                            author_karma = None\n",
    "\n",
    "                        # Store comment information\n",
    "                        comment_data = {\n",
    "                            \"comment_id\": comment.id,\n",
    "                            \"post_id\": submission.id,\n",
    "                            \"comment_text\": comment.body,\n",
    "                            \"comment_score\": comment.score,\n",
    "                            \"comment_created\": comment_time_str,\n",
    "                            \"comment_author\": author,\n",
    "                            \"comment_author_karma\": author_karma,\n",
    "                        }\n",
    "\n",
    "                        # Append comment data to the post\n",
    "                        posts_data[\"comments\"].append(comment_data)\n",
    "\n",
    "                        # Increment comment counter\n",
    "                        comment_counter += 1\n",
    "\n",
    "                # Append post data to the list\n",
    "                posts.append(posts_data)\n",
    "\n",
    "                # Increment post counter\n",
    "                post_counter += 1\n",
    "\n",
    "                # Print progress\n",
    "                print(f\"Processed {post_counter} posts and {comment_counter} comments (Total: {len(seen_posts)}). Subreddit: {subreddit_name}.\", end='\\r')\n",
    "\n",
    "        except prawcore.exceptions.TooManyRequests as e:\n",
    "            print(f\"Rate limit error occurred: {e}\")\n",
    "            print(\"Waiting before retrying...\")\n",
    "            time.sleep(60)\n",
    "        \n",
    "        # Save the data to a JSON file\n",
    "        with open(\"./data/reddit_data.json\", \"w\") as outfile:\n",
    "            json.dump(posts, outfile, indent=4)\n",
    "\n",
    "        # Sleep for 2 seconds to avoid hitting the Reddit API rate limit\n",
    "        time.sleep(2)\n",
    "\n",
    "# Main scraping logic\n",
    "for subreddit_name in subreddits:\n",
    "    if subreddit_name == 'queensgambit':\n",
    "        scrape_subreddit(subreddit_name, limit, use_query=False)  # No query for 'queensgambit'\n",
    "    else:\n",
    "        scrape_subreddit(subreddit_name, limit, use_query=True, query=query)  # Use query for other subreddits\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
