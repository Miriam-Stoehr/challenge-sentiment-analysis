{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Project - Part II (Data Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>created_time</th>\n",
       "      <th>author</th>\n",
       "      <th>author_karma</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1i98h6m</td>\n",
       "      <td>Without question, Chloe was KGB! It was not su...</td>\n",
       "      <td>Chloe was “in bed”, pun intended, with the KGB...</td>\n",
       "      <td>43</td>\n",
       "      <td>2025-01-24 22:54:08</td>\n",
       "      <td>Reasonable-Buy9281</td>\n",
       "      <td>4393.0</td>\n",
       "      <td>https://www.reddit.com/r/queensgambit/comments...</td>\n",
       "      <td>10</td>\n",
       "      <td>queensgambit</td>\n",
       "      <td>[{'post_id': '1i98h6m', 'comment_id': 'm909dm2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1i91w7h</td>\n",
       "      <td>No russian villanization</td>\n",
       "      <td>I really love how this show didn't villanize r...</td>\n",
       "      <td>66</td>\n",
       "      <td>2025-01-24 18:11:37</td>\n",
       "      <td>Emergency-Tie327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://www.reddit.com/r/queensgambit/comments...</td>\n",
       "      <td>1</td>\n",
       "      <td>queensgambit</td>\n",
       "      <td>[{'post_id': '1i91w7h', 'comment_id': 'm9ctg8j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1i6uc7f</td>\n",
       "      <td>Matt and Mike’s acting in the Queens Gambit</td>\n",
       "      <td>Did anyone else think that the acting of the a...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-21 21:48:40</td>\n",
       "      <td>moviequests</td>\n",
       "      <td>2.0</td>\n",
       "      <td>https://www.reddit.com/r/queensgambit/comments...</td>\n",
       "      <td>8</td>\n",
       "      <td>queensgambit</td>\n",
       "      <td>[{'post_id': '1i6uc7f', 'comment_id': 'm8fe51z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1i6lye5</td>\n",
       "      <td>Why did Mrs. Deardorff say “you should be in t...</td>\n",
       "      <td>Why did Mrs. Deardorff say “you should be in t...</td>\n",
       "      <td>42</td>\n",
       "      <td>2025-01-21 16:04:34</td>\n",
       "      <td>moviequests</td>\n",
       "      <td>2.0</td>\n",
       "      <td>https://www.reddit.com/r/queensgambit/comments...</td>\n",
       "      <td>13</td>\n",
       "      <td>queensgambit</td>\n",
       "      <td>[{'post_id': '1i6lye5', 'comment_id': 'm8d9wxl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1i45hpc</td>\n",
       "      <td>What makes Beth a loveable person?</td>\n",
       "      <td>She makes friends, well wishers everywhere she...</td>\n",
       "      <td>32</td>\n",
       "      <td>2025-01-18 11:18:07</td>\n",
       "      <td>purelibran</td>\n",
       "      <td>8952.0</td>\n",
       "      <td>https://www.reddit.com/r/queensgambit/comments...</td>\n",
       "      <td>6</td>\n",
       "      <td>queensgambit</td>\n",
       "      <td>[{'post_id': '1i45hpc', 'comment_id': 'm7tg8u6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                              title  \\\n",
       "0  1i98h6m  Without question, Chloe was KGB! It was not su...   \n",
       "1  1i91w7h                           No russian villanization   \n",
       "2  1i6uc7f        Matt and Mike’s acting in the Queens Gambit   \n",
       "3  1i6lye5  Why did Mrs. Deardorff say “you should be in t...   \n",
       "4  1i45hpc                 What makes Beth a loveable person?   \n",
       "\n",
       "                                                text  score  \\\n",
       "0  Chloe was “in bed”, pun intended, with the KGB...     43   \n",
       "1  I really love how this show didn't villanize r...     66   \n",
       "2  Did anyone else think that the acting of the a...      0   \n",
       "3  Why did Mrs. Deardorff say “you should be in t...     42   \n",
       "4  She makes friends, well wishers everywhere she...     32   \n",
       "\n",
       "          created_time              author  author_karma  \\\n",
       "0  2025-01-24 22:54:08  Reasonable-Buy9281        4393.0   \n",
       "1  2025-01-24 18:11:37    Emergency-Tie327           0.0   \n",
       "2  2025-01-21 21:48:40         moviequests           2.0   \n",
       "3  2025-01-21 16:04:34         moviequests           2.0   \n",
       "4  2025-01-18 11:18:07          purelibran        8952.0   \n",
       "\n",
       "                                                 url  num_comments  \\\n",
       "0  https://www.reddit.com/r/queensgambit/comments...            10   \n",
       "1  https://www.reddit.com/r/queensgambit/comments...             1   \n",
       "2  https://www.reddit.com/r/queensgambit/comments...             8   \n",
       "3  https://www.reddit.com/r/queensgambit/comments...            13   \n",
       "4  https://www.reddit.com/r/queensgambit/comments...             6   \n",
       "\n",
       "      subreddit                                           comments  \n",
       "0  queensgambit  [{'post_id': '1i98h6m', 'comment_id': 'm909dm2...  \n",
       "1  queensgambit  [{'post_id': '1i91w7h', 'comment_id': 'm9ctg8j...  \n",
       "2  queensgambit  [{'post_id': '1i6uc7f', 'comment_id': 'm8fe51z...  \n",
       "3  queensgambit  [{'post_id': '1i6lye5', 'comment_id': 'm8d9wxl...  \n",
       "4  queensgambit  [{'post_id': '1i45hpc', 'comment_id': 'm7tg8u6...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load JSON data\n",
    "with open(\"./oop_data.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Normalize the JSON data\n",
    "posts_df = pd.json_normalize(data)\n",
    "\n",
    "# Normalize comments data\n",
    "comments_data = []\n",
    "for post in data:\n",
    "    for comment in post['comments']:\n",
    "        #comment['post_id'] = post['post_id']\n",
    "        comments_data.append(comment)\n",
    "\n",
    "comments_df = pd.DataFrame(comments_data)\n",
    "\n",
    "# Display DataFrames\n",
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>created_time</th>\n",
       "      <th>author</th>\n",
       "      <th>author_karma</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1i98h6m</td>\n",
       "      <td>m909dm2</td>\n",
       "      <td>I really don't think Alma was poisoned by the ...</td>\n",
       "      <td>81</td>\n",
       "      <td>2025-01-25 00:03:37</td>\n",
       "      <td>ungainlygay</td>\n",
       "      <td>29417.0</td>\n",
       "      <td>queensgambit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1i98h6m</td>\n",
       "      <td>m940hnv</td>\n",
       "      <td>I thought her name was Cleo ?</td>\n",
       "      <td>11</td>\n",
       "      <td>2025-01-25 16:26:11</td>\n",
       "      <td>Smthcool1</td>\n",
       "      <td>102.0</td>\n",
       "      <td>queensgambit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1i98h6m</td>\n",
       "      <td>m90a0pp</td>\n",
       "      <td>Interesting take. Alma would have been suscept...</td>\n",
       "      <td>7</td>\n",
       "      <td>2025-01-25 00:07:07</td>\n",
       "      <td>Fuertebrazos</td>\n",
       "      <td>2861.0</td>\n",
       "      <td>queensgambit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1i98h6m</td>\n",
       "      <td>m8zz2do</td>\n",
       "      <td>Your Alma theory is interesting. And sad. Mayb...</td>\n",
       "      <td>6</td>\n",
       "      <td>2025-01-24 23:08:41</td>\n",
       "      <td>YellowDaisySpider</td>\n",
       "      <td>2677.0</td>\n",
       "      <td>queensgambit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1i98h6m</td>\n",
       "      <td>m91ewjy</td>\n",
       "      <td>That’s a pretty neat theory</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-01-25 04:08:15</td>\n",
       "      <td>SirZacharia</td>\n",
       "      <td>50261.0</td>\n",
       "      <td>queensgambit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id comment_id                                               text  \\\n",
       "0  1i98h6m    m909dm2  I really don't think Alma was poisoned by the ...   \n",
       "1  1i98h6m    m940hnv                      I thought her name was Cleo ?   \n",
       "2  1i98h6m    m90a0pp  Interesting take. Alma would have been suscept...   \n",
       "3  1i98h6m    m8zz2do  Your Alma theory is interesting. And sad. Mayb...   \n",
       "4  1i98h6m    m91ewjy                        That’s a pretty neat theory   \n",
       "\n",
       "   score         created_time             author  author_karma     subreddit  \n",
       "0     81  2025-01-25 00:03:37        ungainlygay       29417.0  queensgambit  \n",
       "1     11  2025-01-25 16:26:11          Smthcool1         102.0  queensgambit  \n",
       "2      7  2025-01-25 00:07:07       Fuertebrazos        2861.0  queensgambit  \n",
       "3      6  2025-01-24 23:08:41  YellowDaisySpider        2677.0  queensgambit  \n",
       "4      2  2025-01-25 04:08:15        SirZacharia       50261.0  queensgambit  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries in posts_df: 525\n",
      "Entries in comments_df: 6656\n"
     ]
    }
   ],
   "source": [
    "print(f\"Entries in posts_df: {posts_df.shape[0]}\")\n",
    "print(f\"Entries in comments_df: {comments_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine posts and comments df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['post_id', 'title', 'text', 'score', 'created_time', 'author', 'author_karma', 'url', 'num_comments', 'subreddit', 'comments']\n",
      "['post_id', 'comment_id', 'text', 'score', 'created_time', 'author', 'author_karma', 'subreddit']\n"
     ]
    }
   ],
   "source": [
    "print(posts_df.columns.tolist())\n",
    "print(comments_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'comments' column from posts_df\n",
    "posts_df.drop('comments', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['post_id', 'title', 'text', 'score', 'created_time', 'author',\n",
       "       'author_karma', 'url', 'num_comments', 'subreddit', 'comment_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the 'posts_df' and 'comments_df' DataFrames\n",
    "combined_df = pd.concat([posts_df, comments_df], ignore_index=True)\n",
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resort columns\n",
    "combined_df = combined_df[['post_id', 'comment_id', 'title', 'text', 'created_time', 'subreddit', 'score', 'num_comments', 'author', 'author_karma', 'url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries: 7181\n"
     ]
    }
   ],
   "source": [
    "print(f\"Entries: {combined_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Empty Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'text' column is empty or only contains whitespace\n",
    "combined_df = combined_df[combined_df['text'].str.strip().notna() & (combined_df['text'].str.strip() != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries: 7180\n"
     ]
    }
   ],
   "source": [
    "print(f\"Entries: {combined_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Entries That Are Too Short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, column):\n",
    "    # Remove entries with no more than three words in \"comment_text\"\n",
    "    df = df[df[column].str.split().str.len() > 12]\n",
    "    return df\n",
    "\n",
    "combined_df = clean_data(combined_df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries: 4923\n"
     ]
    }
   ],
   "source": [
    "print(f\"Entries: {combined_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>created_time</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>author</th>\n",
       "      <th>author_karma</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [post_id, comment_id, title, text, created_time, subreddit, score, num_comments, author, author_karma, url]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there are any duplicates in the 'text' column\n",
    "combined_df[combined_df.duplicated(subset='text')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates in the 'text' column\n",
    "#combined_df.drop_duplicates(subset='text', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Entries: {combined_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary characters from the 'text' column, tokenize the text, and remove stopwords\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_and_remove_stopwords(text: str) -> list:\n",
    "    # Clean the text first (remove URLs, mentions, special characters, etc.)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)  # Remove mentions and hashtags\n",
    "    text = re.sub(r'[^\\w\\s,\\'\\.\\?!]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'[\\U00010000-\\U0010FFFF]', '', text)  # Remove emojis (Unicode range)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    \n",
    "    # Tokenize the text after cleaning\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words from tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "combined_df['tokens'] = combined_df['text'].apply(clean_and_remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Entries: {combined_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens: list) -> list:\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "combined_df['lemmatized_tokens'] = combined_df['tokens'].apply(lemmatize_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
