{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 619 posts and 12570 comments (Total: 13189). Subreddit: NetflixBestOf.\r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import praw\n",
    "import prawcore\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class RedditScraper:\n",
    "    \"\"\"A class to scrape posts and comments from Reddit using PRAW.\"\"\"\n",
    "\n",
    "    def __init__(self, config_file: str, subreddits: List[str], query: str, limit: int, output_file: str):\n",
    "        \"\"\"\n",
    "        Initialize the RedditScraper with API credentials, subreddits, and query parameter.\n",
    "\n",
    "        :param config_file: Path to the config file containing Reddit API credentials.\n",
    "        :param subreddits: List of subreddit names to scrape.\n",
    "        :param query: Query string to search within subreddits.\n",
    "        :param limit: Number of posts to scrape per subreddit.\n",
    "        :param output_file: Path to the output JSON file.\n",
    "        \"\"\"\n",
    "        self.config_file = config_file\n",
    "        self.subreddits = subreddits\n",
    "        self.query = query\n",
    "        self.limit = limit\n",
    "        self.output_file = output_file\n",
    "        self.reddit = self._initialize_reddit_client()\n",
    "        self.seen_posts = set()\n",
    "        self.posts = []\n",
    "        self.post_counter = 0\n",
    "        self.comment_counter = 0\n",
    "\n",
    "    def _initialize_reddit_client(self) -> praw.Reddit:\n",
    "        \"\"\"Load API credentials from the config file and initialize the Reddit client.\"\"\"\n",
    "        if not os.path.exists(self.config_file):\n",
    "            raise FileNotFoundError(f\"Config file {self.config_file} not found.\")\n",
    "        \n",
    "        with open(self.config_file) as file:\n",
    "            config = json.load(file)\n",
    "        \n",
    "        required_keys = ['client_id', 'client_secret', 'user_agent', 'username', 'password']\n",
    "        if not all(key in config for key in required_keys):\n",
    "            raise KeyError(\"API credentials not found in the config file or incomplete.\")\n",
    "        \n",
    "        return praw.Reddit(\n",
    "            client_id=config['client_id'],\n",
    "            client_secret=config['client_secret'],\n",
    "            user_agent=config['user_agent'],\n",
    "            username=config['username'],\n",
    "            password=config['password']\n",
    "        )\n",
    "    \n",
    "    def scrape(self):\n",
    "        \"\"\"Main method to scrape all specified subreddits.\"\"\"\n",
    "        for subreddit_name in self.subreddits:\n",
    "            use_query = subreddit_name != 'queensgambit'\n",
    "            self._scrape_subreddit(subreddit_name, use_query)\n",
    "    \n",
    "    def _scrape_subreddit(self, subreddit_name: str, use_query: bool):\n",
    "        \"\"\"Scrape posts and comments from a specific subreddit.\"\"\"\n",
    "        subreddit = self.reddit.subreddit(subreddit_name)\n",
    "        subreddit_posts = subreddit.search(self.query, limit=self.limit, sort='new') if use_query else subreddit.new(limit=self.limit)\n",
    "\n",
    "        for post in subreddit_posts:\n",
    "            try:\n",
    "                if self._fetch_post(post, subreddit_name):\n",
    "                    self._fetch_comments(post, subreddit_name)\n",
    "                \n",
    "                print(\n",
    "                    f\"Processed {self.post_counter} posts and {self.comment_counter} comments \"\n",
    "                    f\"(Total: {len(self.seen_posts)}). Subreddit: {subreddit_name}.\",\n",
    "                    end='\\r'\n",
    "                )\n",
    "            except prawcore.exceptions.TooManyRequests as e:\n",
    "                print(f\"Rate limit error occurred: {e}. Retrying in 60 seconds...\")\n",
    "                time.sleep(60)\n",
    "            time.sleep(2)  # Avoid hitting Reddit API rate limit\n",
    "    \n",
    "    def _fetch_post(self, post, subreddit_name) -> bool:\n",
    "        \"\"\"Process a Reddit post.\"\"\"\n",
    "        post_identifier = post.selftext.strip().lower()\n",
    "        if post_identifier in self.seen_posts:\n",
    "            return False\n",
    "        \n",
    "        self.seen_posts.add(post_identifier)\n",
    "        created_time = datetime.fromtimestamp(post.created_utc, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        author, author_karma = self._get_author_info(post.author)\n",
    "\n",
    "        post_data = {\n",
    "            \"post_id\": post.id,\n",
    "            \"title\": post.title,\n",
    "            \"text\": post.selftext,\n",
    "            \"score\": post.score,\n",
    "            \"created_time\": created_time,\n",
    "            \"author\": author,\n",
    "            \"author_karma\": author_karma,\n",
    "            \"url\": post.url,\n",
    "            \"num_comments\": post.num_comments,\n",
    "            \"subreddit\": subreddit_name,\n",
    "            \"comments\": []\n",
    "        }\n",
    "        self.posts.append(post_data)\n",
    "        self.post_counter += 1\n",
    "\n",
    "        # Save updated data to JSON\n",
    "        self._save_to_json()\n",
    "        return True\n",
    "    \n",
    "    def _fetch_comments(self, post, subreddit_name):\n",
    "        \"\"\"Fetch comments for a specific Reddit post.\"\"\"\n",
    "        post.comments.replace_more(limit=0)  # Flatten comments\n",
    "        for comment in post.comments.list():\n",
    "            comment_identifier = comment.body.strip().lower()\n",
    "            if comment_identifier in self.seen_posts:\n",
    "                continue\n",
    "\n",
    "            self.seen_posts.add(comment_identifier)\n",
    "            created_time = datetime.fromtimestamp(comment.created_utc, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            author, author_karma = self._get_author_info(comment.author)\n",
    "\n",
    "            comment_data = {\n",
    "                \"post_id\": post.id,\n",
    "                \"comment_id\": comment.id,\n",
    "                \"text\": comment.body,\n",
    "                \"score\": comment.score,\n",
    "                \"created_time\": created_time,\n",
    "                \"author\": author,\n",
    "                \"author_karma\": author_karma,\n",
    "                \"subreddit\": subreddit_name\n",
    "            }\n",
    "            self.posts[-1]['comments'].append(comment_data)\n",
    "            self.comment_counter += 1\n",
    "\n",
    "            # Save updated data to JSON\n",
    "            self._save_to_json()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_author_info(author) -> (Optional[str], Optional[int]):\n",
    "        \"\"\"Retrieve author name and karma, if available.\"\"\"\n",
    "        if not author:\n",
    "            return 'Deleted', None\n",
    "        try:\n",
    "            return author.name, author.comment_karma\n",
    "        except AttributeError:\n",
    "            return author.name, None\n",
    "    \n",
    "    def _save_to_json(self):\n",
    "        \"\"\"Save scraped data to a JSON file.\"\"\"\n",
    "        with open(self.output_file, 'w') as file:\n",
    "            json.dump(self.posts, file, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = RedditScraper(\n",
    "        config_file=\"config.json\",\n",
    "        subreddits=['queensgambit', 'netflix', 'NetflixBestOf', 'television', 'TvShows'],\n",
    "        query=\"Queen's Gambit\",\n",
    "        limit=15000,\n",
    "        output_file=\"oop_data.json\"\n",
    "    )\n",
    "    scraper.scrape()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
